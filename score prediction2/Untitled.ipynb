{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8427f5e7-84e4-434f-97eb-5e014b9b6b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Batters:\n",
      "['SC Ganguly' 'BB McCullum' 'RT Ponting' 'DJ Hussey' 'Mohammad Hafeez'\n",
      " 'R Dravid' 'W Jaffer' 'V Kohli' 'JH Kallis' 'CL White' 'MV Boucher'\n",
      " 'B Akhil' 'AA Noffke' 'P Kumar' 'Z Khan' 'SB Joshi' 'PA Patel'\n",
      " 'ML Hayden' 'MEK Hussey' 'MS Dhoni']\n",
      "\n",
      "Unique Bowlers:\n",
      "['P Kumar' 'Z Khan' 'AA Noffke' 'JH Kallis' 'SB Joshi' 'CL White'\n",
      " 'AB Dinda' 'I Sharma' 'AB Agarkar' 'SC Ganguly' 'LR Shukla' 'B Lee'\n",
      " 'S Sreesanth' 'JR Hopes' 'IK Pathan' 'K Goel' 'PP Chawla' 'WA Mota'\n",
      " 'JDP Oram' 'MS Gony']\n",
      "\n",
      "Unique Teams:\n",
      "['Kolkata Knight Riders' 'Royal Challengers Bangalore'\n",
      " 'Chennai Super Kings' 'Kings XI Punjab' 'Rajasthan Royals'\n",
      " 'Delhi Daredevils' 'Mumbai Indians' 'Deccan Chargers'\n",
      " 'Kochi Tuskers Kerala' 'Pune Warriors' 'Sunrisers Hyderabad'\n",
      " 'Rising Pune Supergiants' 'Gujarat Lions' 'Rising Pune Supergiant'\n",
      " 'Delhi Capitals' 'Punjab Kings' 'Lucknow Super Giants' 'Gujarat Titans'\n",
      " 'Royal Challengers Bengaluru']\n",
      "\n",
      "Unique Venues:\n",
      "['M Chinnaswamy Stadium' 'Punjab Cricket Association Stadium, Mohali'\n",
      " 'Feroz Shah Kotla' 'Wankhede Stadium' 'Eden Gardens'\n",
      " 'Sawai Mansingh Stadium' 'Rajiv Gandhi International Stadium, Uppal'\n",
      " 'MA Chidambaram Stadium, Chepauk' 'Dr DY Patil Sports Academy' 'Newlands'\n",
      " \"St George's Park\" 'Kingsmead' 'SuperSport Park' 'Buffalo Park'\n",
      " 'New Wanderers Stadium' 'De Beers Diamond Oval' 'OUTsurance Oval'\n",
      " 'Brabourne Stadium' 'Sardar Patel Stadium, Motera' 'Barabati Stadium'\n",
      " 'Brabourne Stadium, Mumbai'\n",
      " 'Vidarbha Cricket Association Stadium, Jamtha'\n",
      " 'Himachal Pradesh Cricket Association Stadium' 'Nehru Stadium'\n",
      " 'Holkar Cricket Stadium'\n",
      " 'Dr. Y.S. Rajasekhara Reddy ACA-VDCA Cricket Stadium'\n",
      " 'Subrata Roy Sahara Stadium' 'Maharashtra Cricket Association Stadium'\n",
      " 'Shaheed Veer Narayan Singh International Stadium'\n",
      " 'JSCA International Stadium Complex' 'Sheikh Zayed Stadium'\n",
      " 'Sharjah Cricket Stadium' 'Dubai International Cricket Stadium'\n",
      " 'Punjab Cricket Association IS Bindra Stadium, Mohali'\n",
      " 'Saurashtra Cricket Association Stadium' 'Green Park'\n",
      " 'M.Chinnaswamy Stadium' 'Punjab Cricket Association IS Bindra Stadium'\n",
      " 'Rajiv Gandhi International Stadium' 'MA Chidambaram Stadium'\n",
      " 'Arun Jaitley Stadium' 'MA Chidambaram Stadium, Chepauk, Chennai'\n",
      " 'Wankhede Stadium, Mumbai' 'Narendra Modi Stadium, Ahmedabad'\n",
      " 'Arun Jaitley Stadium, Delhi' 'Zayed Cricket Stadium, Abu Dhabi'\n",
      " 'Dr DY Patil Sports Academy, Mumbai'\n",
      " 'Maharashtra Cricket Association Stadium, Pune' 'Eden Gardens, Kolkata'\n",
      " 'Punjab Cricket Association IS Bindra Stadium, Mohali, Chandigarh'\n",
      " 'Bharat Ratna Shri Atal Bihari Vajpayee Ekana Cricket Stadium, Lucknow'\n",
      " 'Rajiv Gandhi International Stadium, Uppal, Hyderabad'\n",
      " 'M Chinnaswamy Stadium, Bengaluru' 'Barsapara Cricket Stadium, Guwahati'\n",
      " 'Sawai Mansingh Stadium, Jaipur'\n",
      " 'Himachal Pradesh Cricket Association Stadium, Dharamsala'\n",
      " 'Maharaja Yadavindra Singh International Cricket Stadium, Mullanpur'\n",
      " 'Dr. Y.S. Rajasekhara Reddy ACA-VDCA Cricket Stadium, Visakhapatnam']\n",
      "\n",
      "Unique Pitch Types:\n",
      "['Unknown' 'Sluggish' 'Balanced' 'Batting-friendly' 'Spin-friendly']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"final_dataset_with_pitch.csv\")\n",
    "\n",
    "print(\"Unique Batters:\")\n",
    "print(df['batter'].unique()[:20])  # First 20 batters\n",
    "\n",
    "print(\"\\nUnique Bowlers:\")\n",
    "print(df['bowler'].unique()[:20])  # First 20 bowlers\n",
    "\n",
    "print(\"\\nUnique Teams:\")\n",
    "print(df['batting_team'].unique())\n",
    "\n",
    "print(\"\\nUnique Venues:\")\n",
    "print(df['venue'].unique())\n",
    "\n",
    "print(\"\\nUnique Pitch Types:\")\n",
    "print(df['pitch_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18f66ba-f21f-40cd-8c3c-d3027621b72c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priya\\AppData\\Local\\Temp\\ipykernel_15544\\3548767126.py:81: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(['match_id', 'inning'], group_keys=False).apply(calculate_next_over_runs_improved)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Creating proper final score targets without data leakage...\n",
      "✅ Remaining runs range: 0.0 to 287.0\n",
      "✅ Average remaining runs: 84.4\n",
      "✅ Samples after filtering: 257725\n",
      "✅ Final samples: 253713\n",
      "✅ Final feature matrix: (235954, 31)\n",
      "✅ Remaining runs - Mean: 84.7, Std: 48.0\n",
      "✅ Processed features: (188763, 1295)\n",
      "🚀 Training Remaining Runs Model (Fixed Data Leakage)...\n",
      "Epoch 1/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 27ms/step - loss: 46.9637 - mae: 27.9633 - mse: 1741.7047 - val_loss: 18.7947 - val_mae: 16.2857 - val_mse: 481.8070 - learning_rate: 5.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 26ms/step - loss: 19.6793 - mae: 17.2188 - mse: 534.5770 - val_loss: 18.5952 - val_mae: 16.3521 - val_mse: 498.9345 - learning_rate: 5.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 27ms/step - loss: 19.2247 - mae: 16.9899 - mse: 520.1692 - val_loss: 18.1291 - val_mae: 15.9114 - val_mse: 466.2678 - learning_rate: 5.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 28ms/step - loss: 19.0104 - mae: 16.8507 - mse: 515.9432 - val_loss: 18.2455 - val_mae: 16.1958 - val_mse: 470.6215 - learning_rate: 5.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 25ms/step - loss: 18.8932 - mae: 16.8410 - mse: 514.6837 - val_loss: 18.0005 - val_mae: 15.9449 - val_mse: 471.7075 - learning_rate: 5.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 27ms/step - loss: 18.7038 - mae: 16.6714 - mse: 504.5688 - val_loss: 17.8892 - val_mae: 15.9481 - val_mse: 471.6452 - learning_rate: 5.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 18ms/step - loss: 18.5657 - mae: 16.6577 - mse: 507.5904 - val_loss: 17.5659 - val_mae: 15.7575 - val_mse: 464.1489 - learning_rate: 5.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 27ms/step - loss: 18.4338 - mae: 16.5650 - mse: 502.9535 - val_loss: 17.5025 - val_mae: 15.6407 - val_mse: 458.2386 - learning_rate: 5.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 28ms/step - loss: 18.3818 - mae: 16.5686 - mse: 503.1151 - val_loss: 17.4412 - val_mae: 15.7434 - val_mse: 464.4918 - learning_rate: 5.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 29ms/step - loss: 18.1891 - mae: 16.4081 - mse: 494.3319 - val_loss: 17.4027 - val_mae: 15.6565 - val_mse: 456.7860 - learning_rate: 5.0000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 29ms/step - loss: 18.1150 - mae: 16.3768 - mse: 495.1712 - val_loss: 17.3647 - val_mae: 15.6850 - val_mse: 453.5002 - learning_rate: 5.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 29ms/step - loss: 17.9693 - mae: 16.2828 - mse: 486.5045 - val_loss: 17.2381 - val_mae: 15.6183 - val_mse: 461.7743 - learning_rate: 5.0000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 12ms/step - loss: 17.9252 - mae: 16.2664 - mse: 490.0213 - val_loss: 17.2507 - val_mae: 15.5921 - val_mse: 455.3541 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 13ms/step - loss: 17.8061 - mae: 16.2129 - mse: 487.6266 - val_loss: 17.3956 - val_mae: 15.7160 - val_mse: 472.6811 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 13ms/step - loss: 17.9154 - mae: 16.2934 - mse: 489.0700 - val_loss: 17.2365 - val_mae: 15.6612 - val_mse: 464.1415 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 30ms/step - loss: 17.8324 - mae: 16.2643 - mse: 488.9398 - val_loss: 16.9834 - val_mae: 15.4610 - val_mse: 446.6138 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 17.7511 - mae: 16.1947 - mse: 484.7813 - val_loss: 17.0743 - val_mae: 15.5377 - val_mse: 462.5363 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 29ms/step - loss: 17.6808 - mae: 16.1415 - mse: 483.0937 - val_loss: 17.0924 - val_mae: 15.5152 - val_mse: 459.6266 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 29ms/step - loss: 17.6651 - mae: 16.1415 - mse: 484.4112 - val_loss: 16.9375 - val_mae: 15.4710 - val_mse: 456.1770 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 30ms/step - loss: 17.6369 - mae: 16.1419 - mse: 484.8215 - val_loss: 17.3636 - val_mae: 15.9002 - val_mse: 478.4338 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 30ms/step - loss: 17.6026 - mae: 16.1253 - mse: 482.7539 - val_loss: 16.9348 - val_mae: 15.4560 - val_mse: 455.4324 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 17.6282 - mae: 16.1403 - mse: 484.1585 - val_loss: 17.0746 - val_mae: 15.5859 - val_mse: 463.4499 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 31ms/step - loss: 17.5409 - mae: 16.0506 - mse: 477.6480 - val_loss: 17.0334 - val_mae: 15.5527 - val_mse: 462.6169 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 31ms/step - loss: 17.5363 - mae: 16.0667 - mse: 481.2697 - val_loss: 16.8218 - val_mae: 15.3189 - val_mse: 447.5494 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 31ms/step - loss: 17.5258 - mae: 16.0345 - mse: 476.7202 - val_loss: 16.8559 - val_mae: 15.4047 - val_mse: 457.0432 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 32ms/step - loss: 17.3833 - mae: 15.8989 - mse: 472.0557 - val_loss: 16.7672 - val_mae: 15.2886 - val_mse: 443.0029 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 32ms/step - loss: 17.3985 - mae: 15.9240 - mse: 473.0272 - val_loss: 16.7255 - val_mae: 15.2650 - val_mse: 449.7218 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 32ms/step - loss: 17.4327 - mae: 15.9488 - mse: 473.3648 - val_loss: 16.7280 - val_mae: 15.2445 - val_mse: 443.8216 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 32ms/step - loss: 17.3599 - mae: 15.8664 - mse: 470.1509 - val_loss: 16.9033 - val_mae: 15.4123 - val_mse: 453.1649 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 31ms/step - loss: 17.3374 - mae: 15.8340 - mse: 471.7460 - val_loss: 16.7542 - val_mae: 15.2586 - val_mse: 443.2013 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 33ms/step - loss: 17.3075 - mae: 15.8116 - mse: 468.1414 - val_loss: 16.6169 - val_mae: 15.0616 - val_mse: 435.9691 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - loss: 17.2288 - mae: 15.7243 - mse: 463.0623 - val_loss: 16.7407 - val_mae: 15.1999 - val_mse: 442.2527 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 32ms/step - loss: 17.1871 - mae: 15.6700 - mse: 460.3443 - val_loss: 16.8300 - val_mae: 15.2920 - val_mse: 446.0308 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 33ms/step - loss: 17.2762 - mae: 15.7306 - mse: 464.0353 - val_loss: 17.0049 - val_mae: 15.5548 - val_mse: 476.5870 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 34ms/step - loss: 17.1706 - mae: 15.6706 - mse: 461.6124 - val_loss: 17.1077 - val_mae: 15.5592 - val_mse: 477.8988 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 35ms/step - loss: 17.1280 - mae: 15.6113 - mse: 457.1786 - val_loss: 16.7250 - val_mae: 15.1906 - val_mse: 449.4850 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 34ms/step - loss: 17.1911 - mae: 15.6728 - mse: 461.7158 - val_loss: 16.6422 - val_mae: 15.1060 - val_mse: 435.9032 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 35ms/step - loss: 17.1478 - mae: 15.6027 - mse: 457.2596 - val_loss: 16.5903 - val_mae: 15.0460 - val_mse: 443.3085 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 35ms/step - loss: 17.1585 - mae: 15.6283 - mse: 459.3000 - val_loss: 16.7109 - val_mae: 15.1555 - val_mse: 451.0152 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 35ms/step - loss: 17.1191 - mae: 15.5896 - mse: 457.0921 - val_loss: 16.9580 - val_mae: 15.4323 - val_mse: 458.4839 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 36ms/step - loss: 17.0856 - mae: 15.5499 - mse: 455.8902 - val_loss: 16.8869 - val_mae: 15.3314 - val_mse: 457.6355 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 36ms/step - loss: 17.0727 - mae: 15.5184 - mse: 452.9917 - val_loss: 16.8213 - val_mae: 15.2821 - val_mse: 450.3160 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 37ms/step - loss: 16.9586 - mae: 15.4017 - mse: 446.8798 - val_loss: 17.1554 - val_mae: 15.6086 - val_mse: 479.3539 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 47ms/step - loss: 16.9024 - mae: 15.3591 - mse: 445.4616 - val_loss: 16.6501 - val_mae: 15.1222 - val_mse: 437.6406 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 49ms/step - loss: 16.9535 - mae: 15.4069 - mse: 447.5702 - val_loss: 16.6112 - val_mae: 15.0474 - val_mse: 442.2233 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 39ms/step - loss: 16.2652 - mae: 15.2167 - mse: 440.6504 - val_loss: 15.7490 - val_mae: 14.7713 - val_mse: 426.9643 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 39ms/step - loss: 16.0339 - mae: 15.0726 - mse: 430.1412 - val_loss: 15.6262 - val_mae: 14.6700 - val_mse: 426.2446 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 40ms/step - loss: 15.9953 - mae: 15.0348 - mse: 429.8041 - val_loss: 15.8004 - val_mae: 14.8112 - val_mse: 433.6675 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 41ms/step - loss: 16.0932 - mae: 15.1154 - mse: 434.9785 - val_loss: 15.7548 - val_mae: 14.7605 - val_mse: 429.3778 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 42ms/step - loss: 16.0145 - mae: 15.0335 - mse: 429.7001 - val_loss: 15.7556 - val_mae: 14.7769 - val_mse: 431.5340 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 44ms/step - loss: 16.0334 - mae: 15.0376 - mse: 431.9353 - val_loss: 15.8771 - val_mae: 14.8746 - val_mse: 437.7105 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 46ms/step - loss: 15.9730 - mae: 14.9910 - mse: 428.2021 - val_loss: 15.9163 - val_mae: 14.8886 - val_mse: 439.0004 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 57ms/step - loss: 16.0072 - mae: 15.0092 - mse: 430.5682 - val_loss: 15.8495 - val_mae: 14.8682 - val_mse: 438.2865 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 45ms/step - loss: 15.9292 - mae: 14.9555 - mse: 427.3029 - val_loss: 15.6034 - val_mae: 14.6169 - val_mse: 425.2378 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 46ms/step - loss: 15.9673 - mae: 14.9832 - mse: 426.7998 - val_loss: 15.5669 - val_mae: 14.5624 - val_mse: 418.8507 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 47ms/step - loss: 15.9239 - mae: 14.9195 - mse: 425.1736 - val_loss: 15.5775 - val_mae: 14.5949 - val_mse: 421.8829 - learning_rate: 2.5000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 49ms/step - loss: 15.8348 - mae: 14.8329 - mse: 420.1472 - val_loss: 15.7849 - val_mae: 14.7474 - val_mse: 429.9712 - learning_rate: 2.5000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 52ms/step - loss: 15.8618 - mae: 14.8429 - mse: 418.9429 - val_loss: 15.3984 - val_mae: 14.4208 - val_mse: 410.5372 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 53ms/step - loss: 15.8813 - mae: 14.8784 - mse: 425.5129 - val_loss: 15.5073 - val_mae: 14.5115 - val_mse: 418.0604 - learning_rate: 2.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 54ms/step - loss: 15.9395 - mae: 14.9157 - mse: 425.7050 - val_loss: 15.5726 - val_mae: 14.5679 - val_mse: 420.1144 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 52ms/step - loss: 15.9418 - mae: 14.9341 - mse: 426.7393 - val_loss: 15.3936 - val_mae: 14.3788 - val_mse: 404.7211 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 55ms/step - loss: 15.8633 - mae: 14.8424 - mse: 422.8333 - val_loss: 15.8218 - val_mae: 14.7787 - val_mse: 433.3039 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 55ms/step - loss: 15.8733 - mae: 14.8504 - mse: 420.9720 - val_loss: 15.6862 - val_mae: 14.6646 - val_mse: 426.1353 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 58ms/step - loss: 15.8770 - mae: 14.8637 - mse: 422.8702 - val_loss: 15.7143 - val_mae: 14.7028 - val_mse: 428.0558 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 60ms/step - loss: 15.8876 - mae: 14.8544 - mse: 420.6329 - val_loss: 15.4261 - val_mae: 14.3714 - val_mse: 409.3109 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 61ms/step - loss: 15.8433 - mae: 14.8131 - mse: 419.2894 - val_loss: 15.4638 - val_mae: 14.4307 - val_mse: 410.3505 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 62ms/step - loss: 15.7694 - mae: 14.7333 - mse: 415.4362 - val_loss: 15.4011 - val_mae: 14.3768 - val_mse: 410.7430 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 64ms/step - loss: 15.9000 - mae: 14.8557 - mse: 422.9246 - val_loss: 15.5963 - val_mae: 14.5870 - val_mse: 420.0349 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 66ms/step - loss: 15.3504 - mae: 14.6044 - mse: 410.6521 - val_loss: 15.0012 - val_mae: 14.3294 - val_mse: 409.4953 - learning_rate: 1.2500e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 66ms/step - loss: 15.1076 - mae: 14.4345 - mse: 400.7799 - val_loss: 14.9745 - val_mae: 14.3161 - val_mse: 408.0895 - learning_rate: 1.2500e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 67ms/step - loss: 15.2110 - mae: 14.5406 - mse: 406.3319 - val_loss: 14.6613 - val_mae: 13.9816 - val_mse: 390.6248 - learning_rate: 1.2500e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 68ms/step - loss: 15.1670 - mae: 14.4993 - mse: 406.4722 - val_loss: 14.9855 - val_mae: 14.3389 - val_mse: 409.9926 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 70ms/step - loss: 15.1413 - mae: 14.4830 - mse: 404.4995 - val_loss: 14.9758 - val_mae: 14.3139 - val_mse: 408.6548 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 71ms/step - loss: 15.1108 - mae: 14.4464 - mse: 401.3548 - val_loss: 14.8586 - val_mae: 14.2020 - val_mse: 402.2229 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 72ms/step - loss: 15.1575 - mae: 14.4928 - mse: 406.0037 - val_loss: 14.9633 - val_mae: 14.3026 - val_mse: 409.7055 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 72ms/step - loss: 15.0872 - mae: 14.4216 - mse: 398.9473 - val_loss: 15.0444 - val_mae: 14.3834 - val_mse: 412.7191 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 73ms/step - loss: 15.0397 - mae: 14.3730 - mse: 399.3688 - val_loss: 15.2753 - val_mae: 14.5858 - val_mse: 425.7188 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 75ms/step - loss: 15.1551 - mae: 14.4775 - mse: 406.4450 - val_loss: 15.0964 - val_mae: 14.4177 - val_mse: 416.2495 - learning_rate: 1.2500e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 77ms/step - loss: 14.7960 - mae: 14.2957 - mse: 394.5620 - val_loss: 14.6774 - val_mae: 14.2336 - val_mse: 407.4031 - learning_rate: 6.2500e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 76ms/step - loss: 14.6848 - mae: 14.2413 - mse: 391.6247 - val_loss: 14.7449 - val_mae: 14.3250 - val_mse: 411.7142 - learning_rate: 6.2500e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 77ms/step - loss: 14.7279 - mae: 14.2973 - mse: 395.0944 - val_loss: 14.6200 - val_mae: 14.1907 - val_mse: 406.7263 - learning_rate: 6.2500e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 76ms/step - loss: 14.6336 - mae: 14.2088 - mse: 392.7812 - val_loss: 14.5990 - val_mae: 14.1775 - val_mse: 403.3987 - learning_rate: 6.2500e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 32ms/step - loss: 14.6455 - mae: 14.2230 - mse: 394.6938 - val_loss: 14.7357 - val_mae: 14.3194 - val_mse: 412.3127 - learning_rate: 6.2500e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 72ms/step - loss: 14.6033 - mae: 14.1802 - mse: 389.5903 - val_loss: 14.6542 - val_mae: 14.2333 - val_mse: 407.9035 - learning_rate: 6.2500e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 80ms/step - loss: 14.6450 - mae: 14.2236 - mse: 394.1693 - val_loss: 14.6015 - val_mae: 14.1861 - val_mse: 405.2864 - learning_rate: 6.2500e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 81ms/step - loss: 14.6213 - mae: 14.2044 - mse: 392.0821 - val_loss: 14.5068 - val_mae: 14.0889 - val_mse: 400.1123 - learning_rate: 6.2500e-05\n",
      "🚀 Training Next Over Model...\n",
      "Epoch 1/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 71ms/step - loss: 4.4444 - mae: 3.5146 - mse: 19.5048 - val_loss: 3.2867 - val_mae: 3.0909 - val_mse: 15.2296 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 70ms/step - loss: 3.2951 - mae: 3.1217 - mse: 15.0337 - val_loss: 3.1170 - val_mae: 2.9844 - val_mse: 13.9632 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 69ms/step - loss: 3.1642 - mae: 3.0382 - mse: 14.3370 - val_loss: 3.1064 - val_mae: 2.9957 - val_mse: 14.4262 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 71ms/step - loss: 3.1366 - mae: 3.0293 - mse: 14.1983 - val_loss: 3.0681 - val_mae: 2.9675 - val_mse: 13.7227 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 70ms/step - loss: 3.1154 - mae: 3.0158 - mse: 14.0035 - val_loss: 3.0543 - val_mae: 2.9589 - val_mse: 13.3424 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 65ms/step - loss: 3.1024 - mae: 3.0052 - mse: 13.9344 - val_loss: 3.0323 - val_mae: 2.9429 - val_mse: 13.3440 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 64ms/step - loss: 3.0861 - mae: 2.9901 - mse: 13.8421 - val_loss: 3.0194 - val_mae: 2.9298 - val_mse: 13.3225 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 66ms/step - loss: 3.0853 - mae: 2.9929 - mse: 13.8443 - val_loss: 3.0447 - val_mae: 2.9579 - val_mse: 13.7925 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 68ms/step - loss: 3.0734 - mae: 2.9849 - mse: 13.7842 - val_loss: 3.0895 - val_mae: 3.0049 - val_mse: 13.6928 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 67ms/step - loss: 3.0703 - mae: 2.9834 - mse: 13.7871 - val_loss: 2.9966 - val_mae: 2.9143 - val_mse: 13.1299 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 69ms/step - loss: 3.0523 - mae: 2.9685 - mse: 13.6897 - val_loss: 3.0244 - val_mae: 2.9433 - val_mse: 13.7272 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 70ms/step - loss: 3.0560 - mae: 2.9726 - mse: 13.7316 - val_loss: 2.9884 - val_mae: 2.9070 - val_mse: 13.2198 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 71ms/step - loss: 3.0507 - mae: 2.9690 - mse: 13.7094 - val_loss: 2.9804 - val_mae: 2.9051 - val_mse: 13.1298 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 71ms/step - loss: 3.0419 - mae: 2.9621 - mse: 13.6879 - val_loss: 2.9965 - val_mae: 2.9178 - val_mse: 13.3063 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 73ms/step - loss: 3.0473 - mae: 2.9688 - mse: 13.7136 - val_loss: 2.9852 - val_mae: 2.9099 - val_mse: 13.1857 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 74ms/step - loss: 3.0517 - mae: 2.9733 - mse: 13.7639 - val_loss: 3.0178 - val_mae: 2.9424 - val_mse: 13.7310 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 75ms/step - loss: 3.0428 - mae: 2.9660 - mse: 13.7001 - val_loss: 2.9770 - val_mae: 2.9030 - val_mse: 13.1107 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 75ms/step - loss: 3.0427 - mae: 2.9651 - mse: 13.6669 - val_loss: 2.9834 - val_mae: 2.9117 - val_mse: 13.1795 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 75ms/step - loss: 3.0313 - mae: 2.9566 - mse: 13.6433 - val_loss: 2.9722 - val_mae: 2.9003 - val_mse: 13.0468 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 77ms/step - loss: 3.0446 - mae: 2.9698 - mse: 13.7573 - val_loss: 2.9806 - val_mae: 2.9085 - val_mse: 13.2910 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 76ms/step - loss: 3.0374 - mae: 2.9624 - mse: 13.7171 - val_loss: 2.9750 - val_mae: 2.9023 - val_mse: 13.0702 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 78ms/step - loss: 3.0449 - mae: 2.9699 - mse: 13.7302 - val_loss: 2.9850 - val_mae: 2.9137 - val_mse: 13.3893 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 79ms/step - loss: 3.0242 - mae: 2.9497 - mse: 13.5710 - val_loss: 2.9901 - val_mae: 2.9162 - val_mse: 13.4973 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 80ms/step - loss: 3.0099 - mae: 2.9373 - mse: 13.5109 - val_loss: 2.9505 - val_mae: 2.8808 - val_mse: 13.0703 - learning_rate: 7.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 78ms/step - loss: 3.0128 - mae: 2.9411 - mse: 13.5614 - val_loss: 2.9451 - val_mae: 2.8735 - val_mse: 13.1214 - learning_rate: 7.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 79ms/step - loss: 2.9991 - mae: 2.9265 - mse: 13.4901 - val_loss: 2.9537 - val_mae: 2.8819 - val_mse: 13.0501 - learning_rate: 7.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 80ms/step - loss: 2.9994 - mae: 2.9264 - mse: 13.4639 - val_loss: 2.9455 - val_mae: 2.8734 - val_mse: 12.9485 - learning_rate: 7.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 79ms/step - loss: 2.9826 - mae: 2.9084 - mse: 13.3790 - val_loss: 2.9478 - val_mae: 2.8732 - val_mse: 13.0557 - learning_rate: 7.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 81ms/step - loss: 2.9919 - mae: 2.9165 - mse: 13.4529 - val_loss: 2.9521 - val_mae: 2.8757 - val_mse: 13.0519 - learning_rate: 7.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 81ms/step - loss: 2.9883 - mae: 2.9111 - mse: 13.3816 - val_loss: 2.9487 - val_mae: 2.8717 - val_mse: 13.1521 - learning_rate: 7.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 83ms/step - loss: 2.9873 - mae: 2.9086 - mse: 13.4174 - val_loss: 2.9421 - val_mae: 2.8622 - val_mse: 12.9514 - learning_rate: 7.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 83ms/step - loss: 2.9800 - mae: 2.8991 - mse: 13.3498 - val_loss: 2.9277 - val_mae: 2.8477 - val_mse: 13.0420 - learning_rate: 7.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 82ms/step - loss: 2.9865 - mae: 2.9049 - mse: 13.4018 - val_loss: 2.9476 - val_mae: 2.8680 - val_mse: 12.9686 - learning_rate: 7.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 82ms/step - loss: 2.9840 - mae: 2.9036 - mse: 13.3718 - val_loss: 2.9422 - val_mae: 2.8623 - val_mse: 13.0770 - learning_rate: 7.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 84ms/step - loss: 2.9686 - mae: 2.8874 - mse: 13.2512 - val_loss: 2.9321 - val_mae: 2.8516 - val_mse: 12.9017 - learning_rate: 7.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 84ms/step - loss: 2.9796 - mae: 2.8983 - mse: 13.3471 - val_loss: 2.9323 - val_mae: 2.8516 - val_mse: 12.9355 - learning_rate: 7.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 85ms/step - loss: 2.9623 - mae: 2.8798 - mse: 13.2352 - val_loss: 2.9446 - val_mae: 2.8609 - val_mse: 13.0255 - learning_rate: 7.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 86ms/step - loss: 2.9834 - mae: 2.8985 - mse: 13.3695 - val_loss: 2.9417 - val_mae: 2.8582 - val_mse: 13.1030 - learning_rate: 7.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 87ms/step - loss: 2.9740 - mae: 2.8888 - mse: 13.2991 - val_loss: 2.9372 - val_mae: 2.8540 - val_mse: 13.0139 - learning_rate: 7.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 86ms/step - loss: 2.9768 - mae: 2.8921 - mse: 13.3067 - val_loss: 2.9347 - val_mae: 2.8513 - val_mse: 12.8907 - learning_rate: 7.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 86ms/step - loss: 2.9729 - mae: 2.8884 - mse: 13.2397 - val_loss: 2.9336 - val_mae: 2.8502 - val_mse: 13.1309 - learning_rate: 7.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 86ms/step - loss: 2.9812 - mae: 2.8970 - mse: 13.3788 - val_loss: 2.9613 - val_mae: 2.8759 - val_mse: 13.3954 - learning_rate: 7.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 88ms/step - loss: 2.9532 - mae: 2.8703 - mse: 13.1543 - val_loss: 2.9290 - val_mae: 2.8473 - val_mse: 13.0829 - learning_rate: 4.9000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 88ms/step - loss: 2.9266 - mae: 2.8448 - mse: 13.0600 - val_loss: 2.9090 - val_mae: 2.8277 - val_mse: 13.0336 - learning_rate: 4.9000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 88ms/step - loss: 2.9305 - mae: 2.8483 - mse: 13.0868 - val_loss: 2.8914 - val_mae: 2.8092 - val_mse: 12.7825 - learning_rate: 4.9000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 89ms/step - loss: 2.9319 - mae: 2.8492 - mse: 13.1072 - val_loss: 2.8996 - val_mae: 2.8174 - val_mse: 12.7231 - learning_rate: 4.9000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 89ms/step - loss: 2.9228 - mae: 2.8396 - mse: 13.0018 - val_loss: 2.8920 - val_mae: 2.8088 - val_mse: 12.8006 - learning_rate: 4.9000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 88ms/step - loss: 2.9189 - mae: 2.8347 - mse: 12.9893 - val_loss: 2.9013 - val_mae: 2.8161 - val_mse: 12.9413 - learning_rate: 4.9000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 89ms/step - loss: 2.9348 - mae: 2.8493 - mse: 13.1103 - val_loss: 2.8840 - val_mae: 2.7981 - val_mse: 12.7212 - learning_rate: 4.9000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 88ms/step - loss: 2.9216 - mae: 2.8347 - mse: 13.0052 - val_loss: 2.9076 - val_mae: 2.8200 - val_mse: 12.8274 - learning_rate: 4.9000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 90ms/step - loss: 2.9208 - mae: 2.8328 - mse: 12.9822 - val_loss: 2.8981 - val_mae: 2.8106 - val_mse: 12.7704 - learning_rate: 4.9000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 91ms/step - loss: 2.9066 - mae: 2.8182 - mse: 12.9142 - val_loss: 2.8894 - val_mae: 2.8011 - val_mse: 12.7099 - learning_rate: 4.9000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 92ms/step - loss: 2.9134 - mae: 2.8242 - mse: 12.9906 - val_loss: 2.8894 - val_mae: 2.8005 - val_mse: 12.7833 - learning_rate: 4.9000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 92ms/step - loss: 2.9131 - mae: 2.8237 - mse: 12.9745 - val_loss: 2.8835 - val_mae: 2.7952 - val_mse: 12.6698 - learning_rate: 4.9000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 91ms/step - loss: 2.9217 - mae: 2.8322 - mse: 12.9915 - val_loss: 2.8795 - val_mae: 2.7901 - val_mse: 12.6624 - learning_rate: 4.9000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 92ms/step - loss: 2.9218 - mae: 2.8318 - mse: 13.0047 - val_loss: 2.8822 - val_mae: 2.7911 - val_mse: 12.6684 - learning_rate: 4.9000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 92ms/step - loss: 2.9095 - mae: 2.8178 - mse: 12.8830 - val_loss: 2.8869 - val_mae: 2.7946 - val_mse: 12.7807 - learning_rate: 4.9000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 92ms/step - loss: 2.9226 - mae: 2.8291 - mse: 13.0179 - val_loss: 2.8888 - val_mae: 2.7954 - val_mse: 12.6275 - learning_rate: 4.9000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 94ms/step - loss: 2.9169 - mae: 2.8228 - mse: 12.9419 - val_loss: 2.8910 - val_mae: 2.7980 - val_mse: 12.7727 - learning_rate: 4.9000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 94ms/step - loss: 2.9122 - mae: 2.8186 - mse: 12.9223 - val_loss: 2.8935 - val_mae: 2.8008 - val_mse: 12.7215 - learning_rate: 4.9000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 96ms/step - loss: 2.9154 - mae: 2.8218 - mse: 12.9691 - val_loss: 2.8847 - val_mae: 2.7912 - val_mse: 12.6434 - learning_rate: 4.9000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 94ms/step - loss: 2.9060 - mae: 2.8117 - mse: 12.8916 - val_loss: 2.8867 - val_mae: 2.7925 - val_mse: 12.6220 - learning_rate: 4.9000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 94ms/step - loss: 2.9126 - mae: 2.8177 - mse: 12.9391 - val_loss: 2.8771 - val_mae: 2.7834 - val_mse: 12.4788 - learning_rate: 4.9000e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 95ms/step - loss: 2.9047 - mae: 2.8103 - mse: 12.8694 - val_loss: 2.8890 - val_mae: 2.7940 - val_mse: 12.8150 - learning_rate: 4.9000e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 94ms/step - loss: 2.9151 - mae: 2.8198 - mse: 12.9368 - val_loss: 2.8826 - val_mae: 2.7874 - val_mse: 12.6596 - learning_rate: 4.9000e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 97ms/step - loss: 2.8760 - mae: 2.7822 - mse: 12.7301 - val_loss: 2.8524 - val_mae: 2.7615 - val_mse: 12.5449 - learning_rate: 3.4300e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 97ms/step - loss: 2.8598 - mae: 2.7689 - mse: 12.6146 - val_loss: 2.8508 - val_mae: 2.7600 - val_mse: 12.5445 - learning_rate: 3.4300e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 98ms/step - loss: 2.8616 - mae: 2.7708 - mse: 12.6341 - val_loss: 2.8389 - val_mae: 2.7482 - val_mse: 12.5096 - learning_rate: 3.4300e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 96ms/step - loss: 2.8569 - mae: 2.7659 - mse: 12.6058 - val_loss: 2.8417 - val_mae: 2.7508 - val_mse: 12.4793 - learning_rate: 3.4300e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 98ms/step - loss: 2.8420 - mae: 2.7510 - mse: 12.5088 - val_loss: 2.8390 - val_mae: 2.7479 - val_mse: 12.4753 - learning_rate: 3.4300e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 98ms/step - loss: 2.8431 - mae: 2.7516 - mse: 12.5485 - val_loss: 2.8405 - val_mae: 2.7488 - val_mse: 12.5361 - learning_rate: 3.4300e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 98ms/step - loss: 2.8384 - mae: 2.7462 - mse: 12.4856 - val_loss: 2.8374 - val_mae: 2.7449 - val_mse: 12.4120 - learning_rate: 3.4300e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 98ms/step - loss: 2.8459 - mae: 2.7533 - mse: 12.5653 - val_loss: 2.8298 - val_mae: 2.7371 - val_mse: 12.4365 - learning_rate: 3.4300e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 97ms/step - loss: 2.8453 - mae: 2.7523 - mse: 12.5884 - val_loss: 2.8403 - val_mae: 2.7473 - val_mse: 12.5405 - learning_rate: 3.4300e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 100ms/step - loss: 2.8393 - mae: 2.7461 - mse: 12.4875 - val_loss: 2.8302 - val_mae: 2.7374 - val_mse: 12.4557 - learning_rate: 3.4300e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 100ms/step - loss: 2.8508 - mae: 2.7575 - mse: 12.6087 - val_loss: 2.8480 - val_mae: 2.7546 - val_mse: 12.6221 - learning_rate: 3.4300e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 100ms/step - loss: 2.8464 - mae: 2.7527 - mse: 12.5692 - val_loss: 2.8321 - val_mae: 2.7388 - val_mse: 12.4107 - learning_rate: 3.4300e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 100ms/step - loss: 2.8412 - mae: 2.7478 - mse: 12.5394 - val_loss: 2.8254 - val_mae: 2.7323 - val_mse: 12.4428 - learning_rate: 3.4300e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 101ms/step - loss: 2.8311 - mae: 2.7374 - mse: 12.4662 - val_loss: 2.8334 - val_mae: 2.7394 - val_mse: 12.4404 - learning_rate: 3.4300e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 101ms/step - loss: 2.8426 - mae: 2.7485 - mse: 12.5531 - val_loss: 2.8315 - val_mae: 2.7362 - val_mse: 12.4361 - learning_rate: 3.4300e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 101ms/step - loss: 2.8487 - mae: 2.7534 - mse: 12.5618 - val_loss: 2.8291 - val_mae: 2.7343 - val_mse: 12.3770 - learning_rate: 3.4300e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 101ms/step - loss: 2.8331 - mae: 2.7381 - mse: 12.5017 - val_loss: 2.8236 - val_mae: 2.7283 - val_mse: 12.3235 - learning_rate: 3.4300e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 102ms/step - loss: 2.8459 - mae: 2.7503 - mse: 12.5300 - val_loss: 2.8298 - val_mae: 2.7338 - val_mse: 12.5040 - learning_rate: 3.4300e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 103ms/step - loss: 2.8340 - mae: 2.7380 - mse: 12.4496 - val_loss: 2.8166 - val_mae: 2.7206 - val_mse: 12.3383 - learning_rate: 3.4300e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 103ms/step - loss: 2.8177 - mae: 2.7215 - mse: 12.3359 - val_loss: 2.8232 - val_mae: 2.7276 - val_mse: 12.3172 - learning_rate: 3.4300e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 104ms/step - loss: 2.8301 - mae: 2.7339 - mse: 12.4128 - val_loss: 2.8403 - val_mae: 2.7439 - val_mse: 12.4897 - learning_rate: 3.4300e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 105ms/step - loss: 2.8314 - mae: 2.7351 - mse: 12.4201 - val_loss: 2.8270 - val_mae: 2.7304 - val_mse: 12.4087 - learning_rate: 3.4300e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 105ms/step - loss: 2.8385 - mae: 2.7419 - mse: 12.4849 - val_loss: 2.8347 - val_mae: 2.7388 - val_mse: 12.4557 - learning_rate: 3.4300e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 106ms/step - loss: 2.8117 - mae: 2.7166 - mse: 12.3213 - val_loss: 2.8107 - val_mae: 2.7174 - val_mse: 12.4034 - learning_rate: 2.4010e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 107ms/step - loss: 2.7969 - mae: 2.7039 - mse: 12.2708 - val_loss: 2.7995 - val_mae: 2.7078 - val_mse: 12.2687 - learning_rate: 2.4010e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 106ms/step - loss: 2.7861 - mae: 2.6945 - mse: 12.1746 - val_loss: 2.7902 - val_mae: 2.6996 - val_mse: 12.1885 - learning_rate: 2.4010e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 108ms/step - loss: 2.7780 - mae: 2.6876 - mse: 12.1371 - val_loss: 2.7845 - val_mae: 2.6946 - val_mse: 12.1701 - learning_rate: 2.4010e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 109ms/step - loss: 2.7729 - mae: 2.6830 - mse: 12.1034 - val_loss: 2.7766 - val_mae: 2.6868 - val_mse: 12.0911 - learning_rate: 2.4010e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 109ms/step - loss: 2.7792 - mae: 2.6890 - mse: 12.1424 - val_loss: 2.7875 - val_mae: 2.6978 - val_mse: 12.2269 - learning_rate: 2.4010e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 114ms/step - loss: 2.7733 - mae: 2.6835 - mse: 12.1148 - val_loss: 2.7729 - val_mae: 2.6834 - val_mse: 12.0872 - learning_rate: 2.4010e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 109ms/step - loss: 2.7793 - mae: 2.6899 - mse: 12.1849 - val_loss: 2.7847 - val_mae: 2.6954 - val_mse: 12.1306 - learning_rate: 2.4010e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 111ms/step - loss: 2.7674 - mae: 2.6779 - mse: 12.0728 - val_loss: 2.7712 - val_mae: 2.6821 - val_mse: 12.0909 - learning_rate: 2.4010e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - loss: 2.7665 - mae: 2.6775 - mse: 12.0689 - val_loss: 2.7756 - val_mae: 2.6866 - val_mse: 12.1049 - learning_rate: 2.4010e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 111ms/step - loss: 2.7625 - mae: 2.6735 - mse: 12.0595 - val_loss: 2.7884 - val_mae: 2.6994 - val_mse: 12.1443 - learning_rate: 2.4010e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 112ms/step - loss: 2.7744 - mae: 2.6854 - mse: 12.1295 - val_loss: 2.7846 - val_mae: 2.6956 - val_mse: 12.2166 - learning_rate: 2.4010e-04\n",
      "\n",
      "==================================================\n",
      "MODEL EVALUATION\n",
      "==================================================\n",
      "\n",
      "✅ Remaining Runs Model Evaluation:\n",
      "   MAE: 13.95\n",
      "   R² Score: 0.832\n",
      "   Mean Actual Remaining: 84.9\n",
      "   Mean Predicted Remaining: 87.8\n",
      "\n",
      "🏏 Sample Remaining Runs Predictions:\n",
      "   Actual: 17.0, Predicted: 20.6\n",
      "   Actual: 58.0, Predicted: 59.8\n",
      "   Actual: 88.0, Predicted: 96.9\n",
      "   Actual: 107.0, Predicted: 131.2\n",
      "   Actual: 146.0, Predicted: 133.5\n",
      "\n",
      "✅ Next Over Runs Model Evaluation:\n",
      "   MAE: 2.71\n",
      "   R² Score: 0.189\n",
      "\n",
      "✅ Models saved successfully!\n",
      "📋 Summary:\n",
      "   • Remaining Runs MAE: 13.95 (target: <15)\n",
      "   • Remaining Runs R²: 0.832 (target: >0.8)\n",
      "   • Next Over MAE: 2.71 (target: <3.0)\n",
      "\n",
      "🔍 Comprehensive Data Leakage Check:\n",
      "   Sample 1:\n",
      "     Current: 124.0, Actual remaining: 17.0\n",
      "     Predicted remaining: 20.6 (error: 3.6)\n",
      "     Final: 144.6 vs actual 141.0\n",
      "   Sample 2:\n",
      "     Current: 116.0, Actual remaining: 58.0\n",
      "     Predicted remaining: 59.8 (error: 1.8)\n",
      "     Final: 175.8 vs actual 174.0\n",
      "   Sample 3:\n",
      "     Current: 60.0, Actual remaining: 88.0\n",
      "     Predicted remaining: 96.9 (error: 8.9)\n",
      "     Final: 156.9 vs actual 148.0\n",
      "   Sample 4:\n",
      "     Current: 54.0, Actual remaining: 107.0\n",
      "     Predicted remaining: 131.2 (error: 24.2)\n",
      "     Final: 185.2 vs actual 161.0\n",
      "   Sample 5:\n",
      "     Current: 13.0, Actual remaining: 146.0\n",
      "     Predicted remaining: 133.5 (error: 12.5)\n",
      "     Final: 146.5 vs actual 159.0\n",
      "\n",
      "📊 Overall Statistics:\n",
      "   Mean error in remaining runs: 13.95\n",
      "   Mean remaining runs: 84.9\n",
      "   Error as % of mean remaining: 16.4%\n",
      "   ✅ Model seems reasonable but monitor for leakage\n",
      "\n",
      "🎉 Training completed!\n",
      "💡 If data leakage is detected, review all feature calculations carefully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "from tensorflow.keras.losses import Huber\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"final_dataset_with_pitch.csv\")\n",
    "\n",
    "## ENHANCED FEATURE ENGINEERING (Keep your original logic)\n",
    "df = df.sort_values(by=['match_id', 'inning', 'over', 'ball'])\n",
    "\n",
    "# Basic cumulative features\n",
    "df['cumulative_runs'] = df.groupby(['match_id', 'inning'])['total_runs'].cumsum()\n",
    "df['cumulative_wickets'] = df.groupby(['match_id', 'inning'])['is_wicket'].cumsum()\n",
    "\n",
    "# Enhanced features with temporal context\n",
    "df['run_rate'] = df['cumulative_runs'] / (df['over'] + 0.1)\n",
    "df['strike_rate'] = df.groupby(['match_id', 'inning', 'batter'])['total_runs'].cumsum() / \\\n",
    "                   df.groupby(['match_id', 'inning', 'batter']).cumcount().add(1)\n",
    "\n",
    "# Player performance metrics with recent form\n",
    "for player_col in ['batter', 'bowler']:\n",
    "    df[f'{player_col}_avg'] = df.groupby(player_col)['total_runs'].transform(\n",
    "        lambda x: x.expanding().mean().shift(1))\n",
    "    df[f'{player_col}_last5'] = df.groupby(player_col)['total_runs'].transform(\n",
    "        lambda x: x.rolling(30).mean())\n",
    "\n",
    "# Partnership dynamics\n",
    "df['partnership_runs'] = df.groupby(['match_id', 'inning', 'batter', 'non_striker'])['total_runs'].cumsum()\n",
    "df['partnership_balls'] = df.groupby(['match_id', 'inning', 'batter', 'non_striker']).cumcount() + 1\n",
    "df['partnership_momentum'] = df['partnership_runs'] / df['partnership_balls']\n",
    "\n",
    "# Bowler fatigue and recent performance\n",
    "df['bowler_balls_bowled'] = df.groupby(['match_id', 'inning', 'bowler']).cumcount() + 1\n",
    "df['bowler_recent_economy'] = df.groupby(['match_id', 'inning', 'bowler'])['total_runs'].rolling(12, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
    "\n",
    "# Match phase and pressure indicators\n",
    "def get_phase(over):\n",
    "    if over <= 6: return 'Powerplay'\n",
    "    elif over <= 10: return 'Middle1'\n",
    "    elif over <= 15: return 'Middle2'\n",
    "    return 'Death'\n",
    "\n",
    "df['phase'] = df['over'].apply(get_phase)\n",
    "df['is_death_over'] = (df['over'] >= 16).astype(int)\n",
    "df['runs_last_5_overs'] = df.groupby(['match_id', 'inning'])['total_runs'].rolling(30, min_periods=1).sum().reset_index(level=[0,1], drop=True)\n",
    "df['wickets_last_5_overs'] = df.groupby(['match_id', 'inning'])['is_wicket'].rolling(30, min_periods=1).sum().reset_index(level=[0,1], drop=True)\n",
    "\n",
    "# IMPROVED TARGET ENGINEERING for next over (keep your original)\n",
    "def calculate_next_over_runs_improved(group):\n",
    "    group = group.sort_values(['over', 'ball'])\n",
    "    over_runs = []\n",
    "    current_over = -1\n",
    "    current_runs = 0\n",
    "    \n",
    "    for idx, row in group.iterrows():\n",
    "        if row['over'] != current_over:\n",
    "            if current_over >= 0:\n",
    "                over_runs.append((current_over, current_runs))\n",
    "            current_over = row['over']\n",
    "            current_runs = row['total_runs']\n",
    "        else:\n",
    "            current_runs += row['total_runs']\n",
    "    \n",
    "    if current_over >= 0:\n",
    "        over_runs.append((current_over, current_runs))\n",
    "    \n",
    "    over_to_runs = dict(over_runs)\n",
    "    group['next_over_runs'] = group['over'].apply(\n",
    "        lambda x: over_to_runs.get(x + 1, np.nan)\n",
    "    )\n",
    "    \n",
    "    return group\n",
    "\n",
    "df = df.groupby(['match_id', 'inning'], group_keys=False).apply(calculate_next_over_runs_improved)\n",
    "\n",
    "# 🔧 FIXED: No data leakage for final score prediction\n",
    "print(\"🔧 Creating proper final score targets without data leakage...\")\n",
    "\n",
    "# Calculate actual final scores for each match-inning\n",
    "actual_final_scores = df.groupby(['match_id', 'inning'])['total_runs'].sum().reset_index()\n",
    "actual_final_scores.columns = ['match_id', 'inning', 'actual_final_score']\n",
    "\n",
    "# Merge to get the actual final score\n",
    "df = pd.merge(df, actual_final_scores, on=['match_id', 'inning'])\n",
    "\n",
    "# 🎯 KEY FIX: Create target as REMAINING RUNS instead of final score\n",
    "df['remaining_runs'] = df['actual_final_score'] - df['cumulative_runs']\n",
    "\n",
    "print(f\"✅ Remaining runs range: {df['remaining_runs'].min():.1f} to {df['remaining_runs'].max():.1f}\")\n",
    "print(f\"✅ Average remaining runs: {df['remaining_runs'].mean():.1f}\")\n",
    "\n",
    "# Filter out completed innings (remaining_runs <= 0)\n",
    "df = df[df['remaining_runs'] > 0]\n",
    "print(f\"✅ Samples after filtering: {len(df)}\")\n",
    "\n",
    "# Match state features (keep your original logic)\n",
    "df['wickets_remaining'] = 10 - df['cumulative_wickets']\n",
    "df['resources_remaining'] = (20 - df['over']) * (df['wickets_remaining'] / 10)\n",
    "\n",
    "# ⚠️ COMPLETELY REMOVE FEATURES THAT CAUSE DATA LEAKAGE\n",
    "df['overs_remaining'] = 20 - df['over']\n",
    "\n",
    "# 🚫 DO NOT calculate required_run_rate - it causes data leakage\n",
    "# 🚫 DO NOT calculate run_rate_delta - it depends on required_run_rate\n",
    "# These features will be excluded from training\n",
    "\n",
    "# Additional features for next over prediction (keep original)\n",
    "df['balls_faced_current_over'] = df.groupby(['match_id', 'inning', 'over']).cumcount() + 1\n",
    "df['runs_current_over'] = df.groupby(['match_id', 'inning', 'over'])['total_runs'].cumsum()\n",
    "df['current_over_rate'] = df['runs_current_over'] / df['balls_faced_current_over']\n",
    "\n",
    "# Historical over performance by phase\n",
    "df['phase_avg_runs'] = df.groupby(['phase'])['total_runs'].transform('mean')\n",
    "\n",
    "# DATA CLEANING (keep your original approach)\n",
    "df.fillna({\n",
    "    'batter_avg': df['total_runs'].mean(),\n",
    "    'bowler_avg': df['total_runs'].mean(),\n",
    "    'next_over_runs': df.groupby('phase')['total_runs'].transform('mean'),\n",
    "    'bowler_recent_economy': df['total_runs'].mean(),\n",
    "    'batter_last5': df['total_runs'].mean(),\n",
    "    'bowler_last5': df['total_runs'].mean()\n",
    "}, inplace=True)\n",
    "\n",
    "# Remove outliers for remaining runs\n",
    "q1 = df['remaining_runs'].quantile(0.01)\n",
    "q99 = df['remaining_runs'].quantile(0.99)\n",
    "df = df[(df['remaining_runs'] >= q1) & (df['remaining_runs'] <= q99)]\n",
    "\n",
    "print(f\"✅ Final samples: {len(df)}\")\n",
    "\n",
    "# More conservative outlier removal for next over (keep original)\n",
    "next_over_valid = df['next_over_runs'].notna()\n",
    "if next_over_valid.sum() > 0:\n",
    "    q1_over = df.loc[next_over_valid, 'next_over_runs'].quantile(0.05)\n",
    "    q95_over = df.loc[next_over_valid, 'next_over_runs'].quantile(0.95)\n",
    "    df = df[(df['next_over_runs'] >= q1_over) & (df['next_over_runs'] <= q95_over)]\n",
    "\n",
    "# FEATURE SELECTION - REMOVED DATA LEAKAGE FEATURES\n",
    "features = [\n",
    "    'venue', 'batting_team', 'bowling_team', 'batter', 'bowler',\n",
    "    'over', 'cumulative_runs', 'cumulative_wickets', 'phase', 'pitch_type',\n",
    "    'run_rate', 'strike_rate', 'batter_avg', 'bowler_avg',\n",
    "    'partnership_runs', 'partnership_balls', 'bowler_balls_bowled',\n",
    "    'bowler_recent_economy', 'wickets_remaining', 'resources_remaining',\n",
    "    'is_death_over', 'runs_last_5_overs', 'wickets_last_5_overs',\n",
    "    # 🚫 REMOVED: 'required_run_rate', 'run_rate_delta' - cause data leakage\n",
    "    'partnership_momentum',\n",
    "    'batter_last5', 'bowler_last5', 'balls_faced_current_over',\n",
    "    'runs_current_over', 'current_over_rate', 'phase_avg_runs',\n",
    "    'overs_remaining'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y_remaining = df['remaining_runs']  # 🎯 NEW TARGET: remaining runs\n",
    "y_next_over = df['next_over_runs']\n",
    "\n",
    "# Remove rows where next_over_runs is NaN\n",
    "valid_indices = y_next_over.notna()\n",
    "X = X[valid_indices]\n",
    "y_remaining = y_remaining[valid_indices]\n",
    "y_next_over = y_next_over[valid_indices]\n",
    "\n",
    "print(f\"✅ Final feature matrix: {X.shape}\")\n",
    "print(f\"✅ Remaining runs - Mean: {y_remaining.mean():.1f}, Std: {y_remaining.std():.1f}\")\n",
    "\n",
    "# PREPROCESSING (keep original)\n",
    "categorical_features = ['venue', 'batting_team', 'bowling_team', 'batter', 'bowler', 'phase', 'pitch_type']\n",
    "numeric_features = [f for f in features if f not in categorical_features]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "    ('num', StandardScaler(), numeric_features)\n",
    "])\n",
    "\n",
    "# TRAIN-TEST SPLIT\n",
    "X_train, X_test, y_rem_train, y_rem_test, y_next_train, y_next_test = train_test_split(\n",
    "    X, y_remaining, y_next_over, test_size=0.2, random_state=42, stratify=df.loc[valid_indices, 'phase']\n",
    ")\n",
    "\n",
    "# Fit preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "X_train_proc = preprocessor.transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"✅ Processed features: {X_train_proc.shape}\")\n",
    "\n",
    "# 🎯 REMAINING RUNS MODEL (replaces the final score model)\n",
    "def build_remaining_runs_model(input_shape):\n",
    "    \"\"\"Model to predict remaining runs - same architecture as your successful model\"\"\"\n",
    "    inputs = tf.keras.Input(shape=(input_shape,))\n",
    "    \n",
    "    # Use your successful architecture but for remaining runs\n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='swish')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='swish')(x)\n",
    "    outputs = layers.Dense(1)(x)  # Can predict any positive remaining runs\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "        loss=Huber(),  # Same as your successful model\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# NEXT OVER MODEL (keep your improved version)\n",
    "def build_next_over_model_improved(input_shape):\n",
    "    inputs = tf.keras.Input(shape=(input_shape,))\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='relu')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mae',\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# TRAINING CALLBACKS (keep your successful approach)\n",
    "early_stopping_original = callbacks.EarlyStopping(\n",
    "    patience=15,\n",
    "    monitor='val_mae',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr_original = callbacks.ReduceLROnPlateau(\n",
    "    factor=0.5,\n",
    "    patience=7,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "class DynamicWeightAdjuster(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get('val_mae') > logs.get('mae') * 1.1:\n",
    "            lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, lr * 0.9)\n",
    "\n",
    "# 🚀 TRAIN REMAINING RUNS MODEL\n",
    "print(\"🚀 Training Remaining Runs Model (Fixed Data Leakage)...\")\n",
    "remaining_model = build_remaining_runs_model(X_train_proc.shape[1])\n",
    "\n",
    "remaining_history = remaining_model.fit(\n",
    "    X_train_proc, y_rem_train,\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping_original, reduce_lr_original, DynamicWeightAdjuster()],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 🚀 TRAIN NEXT OVER MODEL\n",
    "print(\"🚀 Training Next Over Model...\")\n",
    "over_model = build_next_over_model_improved(X_train_proc.shape[1])\n",
    "\n",
    "early_stopping_conservative = callbacks.EarlyStopping(\n",
    "    patience=20,\n",
    "    monitor='val_mae',\n",
    "    restore_best_weights=True,\n",
    "    min_delta=0.01\n",
    ")\n",
    "\n",
    "reduce_lr_conservative = callbacks.ReduceLROnPlateau(\n",
    "    factor=0.7,\n",
    "    patience=10,\n",
    "    min_lr=1e-5,\n",
    "    min_delta=0.01\n",
    ")\n",
    "\n",
    "over_history = over_model.fit(\n",
    "    X_train_proc, y_next_train,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping_conservative, reduce_lr_conservative],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# POST-PROCESSING (keep your original)\n",
    "def postprocess_next_over_predictions(preds, context_data):\n",
    "    processed_preds = []\n",
    "    \n",
    "    for i, pred in enumerate(preds):\n",
    "        pred_val = pred[0] if hasattr(pred, '__len__') else pred\n",
    "        \n",
    "        over = context_data.iloc[i]['over'] if 'over' in context_data.columns else 10\n",
    "        wickets = context_data.iloc[i]['cumulative_wickets'] if 'cumulative_wickets' in context_data.columns else 2\n",
    "        phase = context_data.iloc[i]['phase'] if 'phase' in context_data.columns else 'Middle1'\n",
    "        \n",
    "        if phase == 'Powerplay':\n",
    "            max_realistic = 18\n",
    "        elif phase == 'Death':\n",
    "            max_realistic = 20\n",
    "        else:\n",
    "            max_realistic = 15\n",
    "        \n",
    "        if wickets >= 8:\n",
    "            pred_val *= 0.8\n",
    "        elif wickets >= 6:\n",
    "            pred_val *= 0.9\n",
    "        \n",
    "        pred_val = max(0, min(pred_val, max_realistic))\n",
    "        processed_preds.append(pred_val)\n",
    "    \n",
    "    return np.array(processed_preds)\n",
    "\n",
    "# EVALUATION\n",
    "def evaluate_model(model, X_test, y_test, model_name, X_test_raw=None):\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    if model_name == \"Next Over Runs Model\" and X_test_raw is not None:\n",
    "        y_pred = postprocess_next_over_predictions(y_pred, X_test_raw)\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n✅ {model_name} Evaluation:\")\n",
    "    print(f\"   MAE: {mae:.2f}\")\n",
    "    print(f\"   R² Score: {r2:.3f}\")\n",
    "    \n",
    "    if model_name == \"Remaining Runs Model\":\n",
    "        print(f\"   Mean Actual Remaining: {np.mean(y_test):.1f}\")\n",
    "        print(f\"   Mean Predicted Remaining: {np.mean(y_pred):.1f}\")\n",
    "        \n",
    "        # Test sample predictions\n",
    "        print(f\"\\n🏏 Sample Remaining Runs Predictions:\")\n",
    "        for i in range(min(5, len(y_test))):\n",
    "            actual = y_test.iloc[i] if hasattr(y_test, 'iloc') else y_test[i]\n",
    "            predicted = y_pred[i][0] if hasattr(y_pred[i], '__len__') else y_pred[i]\n",
    "            print(f\"   Actual: {actual:.1f}, Predicted: {predicted:.1f}\")\n",
    "    \n",
    "    return mae, r2\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "remaining_mae, remaining_r2 = evaluate_model(remaining_model, X_test_proc, y_rem_test, \"Remaining Runs Model\")\n",
    "over_mae, over_r2 = evaluate_model(over_model, X_test_proc, y_next_test, \"Next Over Runs Model\", X_test)\n",
    "\n",
    "# SAVE MODELS WITH NEW NAMES\n",
    "remaining_model.save(\"enhanced_remaining_runs_model.keras\")  # New model\n",
    "over_model.save(\"enhanced_next_over_model.keras\")\n",
    "joblib.dump(preprocessor, \"enhanced_preprocessor.pkl\")\n",
    "\n",
    "print(f\"\\n✅ Models saved successfully!\")\n",
    "print(f\"📋 Summary:\")\n",
    "print(f\"   • Remaining Runs MAE: {remaining_mae:.2f} (target: <15)\")\n",
    "print(f\"   • Remaining Runs R²: {remaining_r2:.3f} (target: >0.8)\")\n",
    "print(f\"   • Next Over MAE: {over_mae:.2f} (target: <3.0)\")\n",
    "\n",
    "# 🔍 ENHANCED Data Leakage Check - More Comprehensive\n",
    "print(f\"\\n🔍 Comprehensive Data Leakage Check:\")\n",
    "\n",
    "# Test multiple samples\n",
    "test_samples = min(5, len(X_test))\n",
    "for i in range(test_samples):\n",
    "    test_sample = X_test.iloc[i:i+1]\n",
    "    test_remaining = y_rem_test.iloc[i] if hasattr(y_rem_test, 'iloc') else y_rem_test[i]\n",
    "    test_current = test_sample['cumulative_runs'].iloc[0]\n",
    "    \n",
    "    predicted_remaining = remaining_model.predict(preprocessor.transform(test_sample), verbose=0)[0][0]\n",
    "    predicted_final = test_current + predicted_remaining\n",
    "    actual_final = test_current + test_remaining\n",
    "    \n",
    "    error_remaining = abs(predicted_remaining - test_remaining)\n",
    "    error_vs_current = abs(predicted_final - test_current)\n",
    "    \n",
    "    print(f\"   Sample {i+1}:\")\n",
    "    print(f\"     Current: {test_current:.1f}, Actual remaining: {test_remaining:.1f}\")\n",
    "    print(f\"     Predicted remaining: {predicted_remaining:.1f} (error: {error_remaining:.1f})\")\n",
    "    print(f\"     Final: {predicted_final:.1f} vs actual {actual_final:.1f}\")\n",
    "\n",
    "# Overall leakage detection\n",
    "all_predictions = remaining_model.predict(X_test_proc, verbose=0).flatten()\n",
    "remaining_errors = np.abs(all_predictions - y_rem_test)\n",
    "mean_error = np.mean(remaining_errors)\n",
    "\n",
    "print(f\"\\n📊 Overall Statistics:\")\n",
    "print(f\"   Mean error in remaining runs: {mean_error:.2f}\")\n",
    "print(f\"   Mean remaining runs: {np.mean(y_rem_test):.1f}\")\n",
    "print(f\"   Error as % of mean remaining: {(mean_error/np.mean(y_rem_test)*100):.1f}%\")\n",
    "\n",
    "if mean_error < 5.0 and remaining_mae < 2.0:\n",
    "    print(\"   ⚠️  LIKELY DATA LEAKAGE - Errors too small for cricket prediction\")\n",
    "    print(\"   🔧 Check your feature engineering for any future information\")\n",
    "elif mean_error < 15.0:\n",
    "    print(\"   ✅ Model seems reasonable but monitor for leakage\")\n",
    "else:\n",
    "    print(\"   ✅ No data leakage detected - model has realistic errors\")\n",
    "\n",
    "print(\"\\n🎉 Training completed!\")\n",
    "print(\"💡 If data leakage is detected, review all feature calculations carefully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f345116f-50dc-4671-bcc0-c3afb491976f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priya\\AppData\\Local\\Temp\\ipykernel_13208\\1011996025.py:80: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(['match_id', 'inning'], group_keys=False).apply(calculate_next_over_runs_improved)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Creating proper final score targets without data leakage...\n",
      "✅ Remaining runs range: 0.0 to 287.0\n",
      "✅ Average remaining runs: 84.4\n",
      "✅ Samples after filtering: 257725\n",
      "✅ Final samples: 253713\n",
      "✅ Final feature matrix: (235954, 31)\n",
      "✅ Remaining runs - Mean: 84.7, Std: 48.0\n",
      "✅ Processed features for XGBoost: (188763, 31)\n",
      "🚀 Training XGBoost Remaining Runs Model...\n",
      "[0]\tvalidation_0-mae:37.21410\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 303\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[38;5;66;03m# Split training data for validation\u001b[39;00m\n\u001b[32m    299\u001b[39m X_train_split, X_val_split, y_rem_train_split, y_rem_val_split = train_test_split(\n\u001b[32m    300\u001b[39m     X_train_proc, y_rem_train, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m\n\u001b[32m    301\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m \u001b[43mremaining_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_rem_train_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_rem_val_split\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Print every 50 rounds\u001b[39;49;00m\n\u001b[32m    307\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# TRAINING NEXT OVER MODEL\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🚀 Training XGBoost Next Over Model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\score prediction2\\scoreenv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\score prediction2\\scoreenv\\Lib\\site-packages\\xgboost\\sklearn.py:1247\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1245\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\score prediction2\\scoreenv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\score prediction2\\scoreenv\\Lib\\site-packages\\xgboost\\training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\score prediction2\\scoreenv\\Lib\\site-packages\\xgboost\\core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"final_dataset_with_pitch.csv\")\n",
    "\n",
    "## ENHANCED FEATURE ENGINEERING (Keep your original logic)\n",
    "df = df.sort_values(by=['match_id', 'inning', 'over', 'ball'])\n",
    "\n",
    "# Basic cumulative features\n",
    "df['cumulative_runs'] = df.groupby(['match_id', 'inning'])['total_runs'].cumsum()\n",
    "df['cumulative_wickets'] = df.groupby(['match_id', 'inning'])['is_wicket'].cumsum()\n",
    "\n",
    "# Enhanced features with temporal context\n",
    "df['run_rate'] = df['cumulative_runs'] / (df['over'] + 0.1)\n",
    "df['strike_rate'] = df.groupby(['match_id', 'inning', 'batter'])['total_runs'].cumsum() / \\\n",
    "                   df.groupby(['match_id', 'inning', 'batter']).cumcount().add(1)\n",
    "\n",
    "# Player performance metrics with recent form\n",
    "for player_col in ['batter', 'bowler']:\n",
    "    df[f'{player_col}_avg'] = df.groupby(player_col)['total_runs'].transform(\n",
    "        lambda x: x.expanding().mean().shift(1))\n",
    "    df[f'{player_col}_last5'] = df.groupby(player_col)['total_runs'].transform(\n",
    "        lambda x: x.rolling(30).mean())\n",
    "\n",
    "# Partnership dynamics\n",
    "df['partnership_runs'] = df.groupby(['match_id', 'inning', 'batter', 'non_striker'])['total_runs'].cumsum()\n",
    "df['partnership_balls'] = df.groupby(['match_id', 'inning', 'batter', 'non_striker']).cumcount() + 1\n",
    "df['partnership_momentum'] = df['partnership_runs'] / df['partnership_balls']\n",
    "\n",
    "# Bowler fatigue and recent performance\n",
    "df['bowler_balls_bowled'] = df.groupby(['match_id', 'inning', 'bowler']).cumcount() + 1\n",
    "df['bowler_recent_economy'] = df.groupby(['match_id', 'inning', 'bowler'])['total_runs'].rolling(12, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
    "\n",
    "# Match phase and pressure indicators\n",
    "def get_phase(over):\n",
    "    if over <= 6: return 'Powerplay'\n",
    "    elif over <= 10: return 'Middle1'\n",
    "    elif over <= 15: return 'Middle2'\n",
    "    return 'Death'\n",
    "\n",
    "df['phase'] = df['over'].apply(get_phase)\n",
    "df['is_death_over'] = (df['over'] >= 16).astype(int)\n",
    "df['runs_last_5_overs'] = df.groupby(['match_id', 'inning'])['total_runs'].rolling(30, min_periods=1).sum().reset_index(level=[0,1], drop=True)\n",
    "df['wickets_last_5_overs'] = df.groupby(['match_id', 'inning'])['is_wicket'].rolling(30, min_periods=1).sum().reset_index(level=[0,1], drop=True)\n",
    "\n",
    "# IMPROVED TARGET ENGINEERING for next over (keep your original)\n",
    "def calculate_next_over_runs_improved(group):\n",
    "    group = group.sort_values(['over', 'ball'])\n",
    "    over_runs = []\n",
    "    current_over = -1\n",
    "    current_runs = 0\n",
    "    \n",
    "    for idx, row in group.iterrows():\n",
    "        if row['over'] != current_over:\n",
    "            if current_over >= 0:\n",
    "                over_runs.append((current_over, current_runs))\n",
    "            current_over = row['over']\n",
    "            current_runs = row['total_runs']\n",
    "        else:\n",
    "            current_runs += row['total_runs']\n",
    "    \n",
    "    if current_over >= 0:\n",
    "        over_runs.append((current_over, current_runs))\n",
    "    \n",
    "    over_to_runs = dict(over_runs)\n",
    "    group['next_over_runs'] = group['over'].apply(\n",
    "        lambda x: over_to_runs.get(x + 1, np.nan)\n",
    "    )\n",
    "    \n",
    "    return group\n",
    "\n",
    "df = df.groupby(['match_id', 'inning'], group_keys=False).apply(calculate_next_over_runs_improved)\n",
    "\n",
    "# 🔧 FIXED: No data leakage for final score prediction\n",
    "print(\"🔧 Creating proper final score targets without data leakage...\")\n",
    "\n",
    "# Calculate actual final scores for each match-inning\n",
    "actual_final_scores = df.groupby(['match_id', 'inning'])['total_runs'].sum().reset_index()\n",
    "actual_final_scores.columns = ['match_id', 'inning', 'actual_final_score']\n",
    "\n",
    "# Merge to get the actual final score\n",
    "df = pd.merge(df, actual_final_scores, on=['match_id', 'inning'])\n",
    "\n",
    "# 🎯 KEY FIX: Create target as REMAINING RUNS instead of final score\n",
    "df['remaining_runs'] = df['actual_final_score'] - df['cumulative_runs']\n",
    "\n",
    "print(f\"✅ Remaining runs range: {df['remaining_runs'].min():.1f} to {df['remaining_runs'].max():.1f}\")\n",
    "print(f\"✅ Average remaining runs: {df['remaining_runs'].mean():.1f}\")\n",
    "\n",
    "# Filter out completed innings (remaining_runs <= 0)\n",
    "df = df[df['remaining_runs'] > 0]\n",
    "print(f\"✅ Samples after filtering: {len(df)}\")\n",
    "\n",
    "# Match state features (keep your original logic)\n",
    "df['wickets_remaining'] = 10 - df['cumulative_wickets']\n",
    "df['resources_remaining'] = (20 - df['over']) * (df['wickets_remaining'] / 10)\n",
    "\n",
    "# ⚠️ COMPLETELY REMOVE FEATURES THAT CAUSE DATA LEAKAGE\n",
    "df['overs_remaining'] = 20 - df['over']\n",
    "\n",
    "# Additional features for next over prediction (keep original)\n",
    "df['balls_faced_current_over'] = df.groupby(['match_id', 'inning', 'over']).cumcount() + 1\n",
    "df['runs_current_over'] = df.groupby(['match_id', 'inning', 'over'])['total_runs'].cumsum()\n",
    "df['current_over_rate'] = df['runs_current_over'] / df['balls_faced_current_over']\n",
    "\n",
    "# Historical over performance by phase\n",
    "df['phase_avg_runs'] = df.groupby(['phase'])['total_runs'].transform('mean')\n",
    "\n",
    "# DATA CLEANING (keep your original approach)\n",
    "df.fillna({\n",
    "    'batter_avg': df['total_runs'].mean(),\n",
    "    'bowler_avg': df['total_runs'].mean(),\n",
    "    'next_over_runs': df.groupby('phase')['total_runs'].transform('mean'),\n",
    "    'bowler_recent_economy': df['total_runs'].mean(),\n",
    "    'batter_last5': df['total_runs'].mean(),\n",
    "    'bowler_last5': df['total_runs'].mean()\n",
    "}, inplace=True)\n",
    "\n",
    "# Remove outliers for remaining runs\n",
    "q1 = df['remaining_runs'].quantile(0.01)\n",
    "q99 = df['remaining_runs'].quantile(0.99)\n",
    "df = df[(df['remaining_runs'] >= q1) & (df['remaining_runs'] <= q99)]\n",
    "\n",
    "print(f\"✅ Final samples: {len(df)}\")\n",
    "\n",
    "# More conservative outlier removal for next over (keep original)\n",
    "next_over_valid = df['next_over_runs'].notna()\n",
    "if next_over_valid.sum() > 0:\n",
    "    q1_over = df.loc[next_over_valid, 'next_over_runs'].quantile(0.05)\n",
    "    q95_over = df.loc[next_over_valid, 'next_over_runs'].quantile(0.95)\n",
    "    df = df[(df['next_over_runs'] >= q1_over) & (df['next_over_runs'] <= q95_over)]\n",
    "\n",
    "# FEATURE SELECTION - REMOVED DATA LEAKAGE FEATURES\n",
    "features = [\n",
    "    'venue', 'batting_team', 'bowling_team', 'batter', 'bowler',\n",
    "    'over', 'cumulative_runs', 'cumulative_wickets', 'phase', 'pitch_type',\n",
    "    'run_rate', 'strike_rate', 'batter_avg', 'bowler_avg',\n",
    "    'partnership_runs', 'partnership_balls', 'bowler_balls_bowled',\n",
    "    'bowler_recent_economy', 'wickets_remaining', 'resources_remaining',\n",
    "    'is_death_over', 'runs_last_5_overs', 'wickets_last_5_overs',\n",
    "    'partnership_momentum',\n",
    "    'batter_last5', 'bowler_last5', 'balls_faced_current_over',\n",
    "    'runs_current_over', 'current_over_rate', 'phase_avg_runs',\n",
    "    'overs_remaining'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y_remaining = df['remaining_runs']  # 🎯 NEW TARGET: remaining runs\n",
    "y_next_over = df['next_over_runs']\n",
    "\n",
    "# Remove rows where next_over_runs is NaN\n",
    "valid_indices = y_next_over.notna()\n",
    "X = X[valid_indices]\n",
    "y_remaining = y_remaining[valid_indices]\n",
    "y_next_over = y_next_over[valid_indices]\n",
    "\n",
    "print(f\"✅ Final feature matrix: {X.shape}\")\n",
    "print(f\"✅ Remaining runs - Mean: {y_remaining.mean():.1f}, Std: {y_remaining.std():.1f}\")\n",
    "\n",
    "# PREPROCESSING - MODIFIED FOR XGBOOST\n",
    "# XGBoost can handle categorical features directly, but let's encode for consistency\n",
    "categorical_features = ['venue', 'batting_team', 'bowling_team', 'batter', 'bowler', 'phase', 'pitch_type']\n",
    "numeric_features = [f for f in features if f not in categorical_features]\n",
    "\n",
    "# For XGBoost, we'll use LabelEncoder instead of OneHotEncoder to reduce dimensionality\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def create_xgboost_preprocessor():\n",
    "    \"\"\"Create a custom preprocessor optimized for XGBoost\"\"\"\n",
    "    label_encoders = {}\n",
    "    \n",
    "    def fit_transform(X):\n",
    "        X_encoded = X.copy()\n",
    "        \n",
    "        # Encode categorical features\n",
    "        for col in categorical_features:\n",
    "            if col in X_encoded.columns:\n",
    "                le = LabelEncoder()\n",
    "                X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "                label_encoders[col] = le\n",
    "        \n",
    "        # Scale only numeric features (XGBoost doesn't strictly need this, but can help)\n",
    "        scaler = StandardScaler()\n",
    "        X_encoded[numeric_features] = scaler.fit_transform(X_encoded[numeric_features])\n",
    "        \n",
    "        return X_encoded, label_encoders, scaler\n",
    "    \n",
    "    def transform(X, label_encoders, scaler):\n",
    "        X_encoded = X.copy()\n",
    "        \n",
    "        # Encode categorical features\n",
    "        for col in categorical_features:\n",
    "            if col in X_encoded.columns and col in label_encoders:\n",
    "                # Handle unknown categories\n",
    "                le = label_encoders[col]\n",
    "                X_encoded[col] = X_encoded[col].astype(str)\n",
    "                \n",
    "                # Map unknown values to a default (most frequent class)\n",
    "                unknown_mask = ~X_encoded[col].isin(le.classes_)\n",
    "                if unknown_mask.any():\n",
    "                    most_frequent = le.classes_[0]  # Use first class as default\n",
    "                    X_encoded.loc[unknown_mask, col] = most_frequent\n",
    "                \n",
    "                X_encoded[col] = le.transform(X_encoded[col])\n",
    "        \n",
    "        # Scale numeric features\n",
    "        X_encoded[numeric_features] = scaler.transform(X_encoded[numeric_features])\n",
    "        \n",
    "        return X_encoded\n",
    "    \n",
    "    return fit_transform, transform\n",
    "\n",
    "# TRAIN-TEST SPLIT\n",
    "X_train, X_test, y_rem_train, y_rem_test, y_next_train, y_next_test = train_test_split(\n",
    "    X, y_remaining, y_next_over, test_size=0.2, random_state=42, stratify=df.loc[valid_indices, 'phase']\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "fit_transform_func, transform_func = create_xgboost_preprocessor()\n",
    "X_train_proc, label_encoders, scaler = fit_transform_func(X_train)\n",
    "X_test_proc = transform_func(X_test, label_encoders, scaler)\n",
    "\n",
    "print(f\"✅ Processed features for XGBoost: {X_train_proc.shape}\")\n",
    "\n",
    "# 🚀 XGBOOST REMAINING RUNS MODEL\n",
    "def create_remaining_runs_xgboost():\n",
    "    \"\"\"XGBoost model optimized for remaining runs prediction\"\"\"\n",
    "    model = XGBRegressor(\n",
    "        # Core parameters\n",
    "        n_estimators=1000,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.1,\n",
    "        \n",
    "        # Regularization\n",
    "        reg_alpha=0.1,     # L1 regularization\n",
    "        reg_lambda=1.0,    # L2 regularization\n",
    "        gamma=0.1,         # Minimum split loss\n",
    "        \n",
    "        # Sampling\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        colsample_bylevel=0.8,\n",
    "        \n",
    "        # Performance\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        \n",
    "        # Early stopping will be handled in fit()\n",
    "        early_stopping_rounds=50,\n",
    "        \n",
    "        # Evaluation metric\n",
    "        eval_metric='mae'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 🚀 XGBOOST NEXT OVER MODEL  \n",
    "def create_next_over_xgboost():\n",
    "    \"\"\"XGBoost model optimized for next over runs prediction\"\"\"\n",
    "    model = XGBRegressor(\n",
    "        # Smaller model for next over (simpler task)\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.15,\n",
    "        \n",
    "        # Regularization\n",
    "        reg_alpha=0.05,\n",
    "        reg_lambda=0.5,\n",
    "        gamma=0.05,\n",
    "        \n",
    "        # Sampling\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        \n",
    "        # Performance\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping_rounds=30,\n",
    "        \n",
    "        # Evaluation metric\n",
    "        eval_metric='mae'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# TRAINING REMAINING RUNS MODEL\n",
    "print(\"🚀 Training XGBoost Remaining Runs Model...\")\n",
    "remaining_model = create_remaining_runs_xgboost()\n",
    "\n",
    "# Split training data for validation\n",
    "X_train_split, X_val_split, y_rem_train_split, y_rem_val_split = train_test_split(\n",
    "    X_train_proc, y_rem_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "remaining_model.fit(\n",
    "    X_train_split, y_rem_train_split,\n",
    "    eval_set=[(X_val_split, y_rem_val_split)],\n",
    "    verbose=50  # Print every 50 rounds\n",
    ")\n",
    "\n",
    "# TRAINING NEXT OVER MODEL\n",
    "print(\"🚀 Training XGBoost Next Over Model...\")\n",
    "over_model = create_next_over_xgboost()\n",
    "\n",
    "X_train_over_split, X_val_over_split, y_next_train_split, y_next_val_split = train_test_split(\n",
    "    X_train_proc, y_next_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "over_model.fit(\n",
    "    X_train_over_split, y_next_train_split,\n",
    "    eval_set=[(X_val_over_split, y_next_val_split)],\n",
    "    verbose=25  # Print every 25 rounds\n",
    ")\n",
    "\n",
    "# POST-PROCESSING (keep your original)\n",
    "def postprocess_next_over_predictions(preds, context_data):\n",
    "    processed_preds = []\n",
    "    \n",
    "    for i, pred in enumerate(preds):\n",
    "        pred_val = pred if not hasattr(pred, '__len__') else pred\n",
    "        \n",
    "        over = context_data.iloc[i]['over'] if 'over' in context_data.columns else 10\n",
    "        wickets = context_data.iloc[i]['cumulative_wickets'] if 'cumulative_wickets' in context_data.columns else 2\n",
    "        phase = context_data.iloc[i]['phase'] if 'phase' in context_data.columns else 'Middle1'\n",
    "        \n",
    "        if phase == 'Powerplay':\n",
    "            max_realistic = 18\n",
    "        elif phase == 'Death':\n",
    "            max_realistic = 20\n",
    "        else:\n",
    "            max_realistic = 15\n",
    "        \n",
    "        if wickets >= 8:\n",
    "            pred_val *= 0.8\n",
    "        elif wickets >= 6:\n",
    "            pred_val *= 0.9\n",
    "        \n",
    "        pred_val = max(0, min(pred_val, max_realistic))\n",
    "        processed_preds.append(pred_val)\n",
    "    \n",
    "    return np.array(processed_preds)\n",
    "\n",
    "# EVALUATION - SIMPLIFIED\n",
    "def evaluate_xgboost_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"✅ {model_name}:\")\n",
    "    print(f\"   MAE: {mae:.2f}\")\n",
    "    print(f\"   R²: {r2:.3f}\")\n",
    "    \n",
    "    return mae, r2\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "remaining_mae, remaining_r2 = evaluate_xgboost_model(remaining_model, X_test_proc, y_rem_test, \"XGBoost Remaining Runs\")\n",
    "over_mae, over_r2 = evaluate_xgboost_model(over_model, X_test_proc, y_next_test, \"XGBoost Next Over\")\n",
    "\n",
    "# TOP 5 FEATURE IMPORTANCE ONLY\n",
    "importance_remaining = remaining_model.feature_importances_\n",
    "feature_names = X_train_proc.columns.tolist()\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance_remaining\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n🎯 Top 5 Most Important Features:\")\n",
    "for i, row in importance_df.head(5).iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "# SAVE MODELS\n",
    "remaining_model.save_model(\"xgboost_remaining_runs_model.json\")\n",
    "over_model.save_model(\"xgboost_next_over_model.json\")\n",
    "\n",
    "preprocessing_data = {\n",
    "    'label_encoders': label_encoders,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': feature_names,\n",
    "    'categorical_features': categorical_features,\n",
    "    'numeric_features': numeric_features\n",
    "}\n",
    "joblib.dump(preprocessing_data, \"xgboost_preprocessor.pkl\")\n",
    "\n",
    "print(f\"\\n📋 FINAL COMPARISON:\")\n",
    "print(f\"   ANN Remaining Runs MAE: 13.95\")\n",
    "print(f\"   XGBoost Remaining Runs MAE: {remaining_mae:.2f}\")\n",
    "if remaining_mae < 13.95:\n",
    "    improvement = ((13.95 - remaining_mae) / 13.95) * 100\n",
    "    print(f\"   🏆 XGBoost is {improvement:.1f}% better than ANN!\")\n",
    "else:\n",
    "    print(f\"   ⚠️  ANN performed better by {remaining_mae - 13.95:.2f} MAE\")\n",
    "\n",
    "print(f\"\\n✅ XGBoost models saved successfully!\")\n",
    "\n",
    "# Check for data leakage - SIMPLIFIED\n",
    "all_predictions = remaining_model.predict(X_test_proc)\n",
    "remaining_errors = np.abs(all_predictions - y_rem_test)\n",
    "mean_error = np.mean(remaining_errors)\n",
    "\n",
    "if mean_error < 5.0:\n",
    "    print(\"   ⚠️  Possible data leakage - very low errors\")\n",
    "else:\n",
    "    print(\"   ✅ No data leakage detected\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea71abe-d815-4966-babf-4fcf19f0d60f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'postprocess_next_over_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m accuracy=\u001b[43mpostprocess_next_over_predictions\u001b[49m.evaluate(X_test,Y_test)\n",
      "\u001b[31mNameError\u001b[39m: name 'postprocess_next_over_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy=postprocess_next_over_predictions.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54ec0e-3a0f-43e5-b786-2016b529fcff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
